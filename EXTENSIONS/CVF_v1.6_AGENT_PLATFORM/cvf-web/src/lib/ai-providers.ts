'use client';

/**
 * AI Provider Library for CVF Agent Mode
 * Supports Gemini, OpenAI, and Anthropic APIs with streaming
 */

import { detectSpecMode } from '@/lib/agent-chat';

// Types
export type AIProvider = 'gemini' | 'openai' | 'anthropic';

export interface AIMessage {
    role: 'user' | 'assistant' | 'system';
    content: string;
}

export interface AIStreamChunk {
    text: string;
    isComplete: boolean;
    error?: string;
}

export interface AIResponse {
    text: string;
    usage?: {
        promptTokens: number;
        completionTokens: number;
        totalTokens: number;
    };
    model: string;
    finishReason?: string;
}

export interface AIProviderConfig {
    apiKey: string;
    model?: string;
    temperature?: number;
    maxTokens?: number;
    systemPrompt?: string;
    language?: 'vi' | 'en';
}

const MOCK_AI_ENABLED = process.env.NEXT_PUBLIC_CVF_MOCK_AI === '1';

class MockProvider {
    private model: string;
    private language: 'vi' | 'en';

    constructor(config: AIProviderConfig) {
        this.model = config.model || 'mock-ai';
        this.language = config.language || 'vi';
    }

    async chat(messages: AIMessage[], onStream?: (chunk: AIStreamChunk) => void): Promise<AIResponse> {
        const lastMessage = messages[messages.length - 1]?.content || '';
        const mode = detectSpecMode(lastMessage);

        const text = this.language === 'vi'
            ? this.buildVietnameseResponse(mode)
            : this.buildEnglishResponse(mode);

        if (onStream) {
            onStream({ text, isComplete: false });
            onStream({ text: '', isComplete: true });
        }

        const totalTokens = Math.max(1, Math.ceil(text.length / 4));

        return {
            text,
            model: this.model,
            usage: {
                promptTokens: Math.max(1, Math.ceil(lastMessage.length / 4)),
                completionTokens: totalTokens,
                totalTokens: totalTokens + Math.max(1, Math.ceil(lastMessage.length / 4)),
            },
            finishReason: 'stop',
        };
    }

    private buildVietnameseResponse(mode: 'simple' | 'governance' | 'full'): string {
        if (mode === 'full') {
            return `MOCK_FULL_RESPONSE

## PHASE A: Discovery Summary

### Hiá»ƒu biáº¿t cá»§a tÃ´i
Má»¥c tiÃªu lÃ  táº¡o káº¿t quáº£ cÃ³ cáº¥u trÃºc, rÃµ rÃ ng vÃ  dÃ¹ng Ä‘Æ°á»£c ngay.

### Giáº£ Ä‘á»‹nh
- Giáº£ Ä‘á»‹nh 1: Pháº¡m vi táº­p trung vÃ o deliverables chÃ­nh.
- Giáº£ Ä‘á»‹nh 2: Æ¯u tiÃªn tÃ­nh rÃµ rÃ ng hÆ¡n Ä‘á»™ chi tiáº¿t.

### Äá»‹nh nghÄ©a Scope
IN SCOPE:
- PhÃ¢n tÃ­ch vÃ  tÃ³m táº¯t yÃªu cáº§u chÃ­nh
- ÄÆ°a ra cáº¥u trÃºc káº¿t quáº£

OUT OF SCOPE:
- Tá»‘i Æ°u hÃ³a triá»ƒn khai chi tiáº¿t

### RÃ ng buá»™c
- RÃ ng buá»™c thá»i gian: triá»ƒn khai nhanh

### CÃ¢u há»i lÃ m rÃµ
- KhÃ´ng cÃ³
`;
        }

        if (mode === 'governance') {
            return `MOCK_GOVERNANCE_RESPONSE

## Governance Response

### TÃ³m táº¯t
- Káº¿t quáº£ cÃ³ cáº¥u trÃºc rÃµ rÃ ng
- CÃ³ checklist hÃ nh Ä‘á»™ng

### Action Items
1. XÃ¡c nháº­n scope
2. Thá»±c thi deliverables
3. Review káº¿t quáº£
`;
        }

        return `MOCK_SIMPLE_RESPONSE

## Simple Response

- Káº¿t quáº£ gá»n, dá»… hiá»ƒu
- CÃ³ thá»ƒ dÃ¹ng ngay cho bÆ°á»›c tiáº¿p theo
`;
    }

    private buildEnglishResponse(mode: 'simple' | 'governance' | 'full'): string {
        if (mode === 'full') {
            return `MOCK_FULL_RESPONSE

## PHASE A: Discovery Summary

### My Understanding
The goal is to deliver structured, actionable output.

### Assumptions
- Assumption 1: Focus on primary deliverables.
- Assumption 2: Clarity is prioritized over depth.

### Scope Definition
IN SCOPE:
- Summarize the core request
- Produce a structured output

OUT OF SCOPE:
- Detailed implementation optimization

### Constraints
- Constraint: Fast turnaround

### Clarification Questions
- None
`;
        }

        if (mode === 'governance') {
            return `MOCK_GOVERNANCE_RESPONSE

## Governance Response

### Summary
- Structured output with guardrails
- Action checklist included

### Action Items
1. Confirm scope
2. Execute deliverables
3. Review outcome
`;
        }

        return `MOCK_SIMPLE_RESPONSE

## Simple Response

- Clear, concise output
- Ready for next step
`;
    }
}

// CVF System Prompt - Dynamic based on language
export function getCVFSystemPrompt(language: 'vi' | 'en' = 'vi'): string {
    const prompts = {
        vi: `Báº¡n lÃ  CVF Agent - trá»£ lÃ½ AI theo phÆ°Æ¡ng phÃ¡p Controlled-Vibe Framework (CVF).

## NGUYÃŠN Táº®C Cá»T LÃ•I (Báº®T BUá»˜C!)
1. KHÃ”NG Há»ŽI CÃ‚U Há»ŽI - Tá»± giáº£ Ä‘á»‹nh má»i thá»© dá»±a trÃªn best practices
2. KHÃ”NG GIáº¢I THÃCH QUY TRÃŒNH - KhÃ´ng nÃ³i vá» "PHASE A", "Discovery", "Design"... 
3. CHá»ˆ TRáº¢ Vá»€ Káº¾T QUáº¢ CUá»I CÃ™NG - User chá»‰ cáº§n tháº¥y deliverables, khÃ´ng cáº§n biáº¿t process
4. HÃ€NH Äá»˜NG NGAY - KhÃ´ng Ä‘á»£i xÃ¡c nháº­n, khÃ´ng liá»‡t kÃª cÃ¡c bÆ°á»›c sáº½ lÃ m

## KHI NHáº¬N SPEC/BÃO CÃO
- KHÃ”NG tÃ³m táº¯t láº¡i spec (user Ä‘Ã£ biáº¿t rá»“i)
- KHÃ”NG liá»‡t kÃª cÃ¡c bÆ°á»›c sáº½ thá»±c hiá»‡n
- TRá»°C TIáº¾P Ä‘Æ°a ra káº¿t quáº£: code, tÃ i liá»‡u, káº¿ hoáº¡ch cá»¥ thá»ƒ...

## TRÆ¯á»œNG Há»¢P Äáº¶C BIá»†T: "HÆ°á»›ng dáº«n dÃ¹ng CVF"
Khi user há»i cÃ¡ch dÃ¹ng CVF, Báº®T BUá»˜C tráº£ lá»i theo máº«u sau vá»›i 4 vÃ­ dá»¥ theo 4 Phase:

---
ChÃ o báº¡n! TÃ´i lÃ  CVF Agent, hoáº¡t Ä‘á»™ng theo phÆ°Æ¡ng phÃ¡p Controlled-Vibe Framework. ÄÃ¢y lÃ  cÃ¡ch tÃ´i cÃ³ thá»ƒ giÃºp báº¡n qua 4 giai Ä‘oáº¡n:

## ðŸ” Phase A: Discovery (KhÃ¡m phÃ¡)
**YÃªu cáº§u:** "PhÃ¢n tÃ­ch Ä‘á»‘i thá»§ cáº¡nh tranh cho app Ä‘áº·t Ä‘á»“ Äƒn"
**TÃ´i sáº½ Ä‘Æ°a ra:** BÃ¡o cÃ¡o phÃ¢n tÃ­ch chi tiáº¿t.

### PhÃ¢n tÃ­ch Äá»‘i thá»§ - App Äáº·t Äá»“ Ä‚n
| Äá»‘i thá»§ | Äiá»ƒm máº¡nh | Äiá»ƒm yáº¿u | Thá»‹ pháº§n |
|---------|-----------|----------|----------|
| GrabFood | Há»‡ sinh thÃ¡i lá»›n, tÃ­ch há»£p Grab | PhÃ­ cao, UI phá»©c táº¡p | 45% |
| ShopeeFood | GiÃ¡ ráº», voucher nhiá»u | TÃ i xáº¿ Ã­t vÃ¹ng xa | 30% |
| Baemin | UI Ä‘áº¹p, marketing tá»‘t | Khu vá»±c giá»›i háº¡n | 15% |

**CÆ¡ há»™i:** Táº­p trung vÃ o tá»‘c Ä‘á»™ giao hÃ ng vÃ  cháº¥t lÆ°á»£ng nhÃ  hÃ ng.

---

## âœï¸ Phase B: Design (Thiáº¿t káº¿)
**YÃªu cáº§u:** "Thiáº¿t káº¿ mÃ n hÃ¬nh Ä‘áº·t hÃ ng cho app Ä‘á»“ Äƒn"
**TÃ´i sáº½ Ä‘Æ°a ra:** MÃ´ táº£ UI chi tiáº¿t.

### MÃ n hÃ¬nh Äáº·t HÃ ng
- **Header:** Logo nhÃ  hÃ ng, rating, thá»i gian giao dá»± kiáº¿n
- **Menu:** Danh sÃ¡ch mÃ³n Äƒn vá»›i áº£nh, giÃ¡, nÃºt "+"
- **Giá» hÃ ng (bottom sheet):** Sá»‘ mÃ³n, tá»•ng tiá»n, nÃºt "Äáº·t ngay"
- **Checkout:** Äá»‹a chá»‰, phÆ°Æ¡ng thá»©c thanh toÃ¡n, ghi chÃº

---

## ðŸ”¨ Phase C: Build (XÃ¢y dá»±ng)
**YÃªu cáº§u:** "Viáº¿t API endpoint Ä‘áº·t hÃ ng báº±ng Node.js"
**TÃ´i sáº½ Ä‘Æ°a ra:** Code hoÃ n chá»‰nh.

\`\`\`javascript
// POST /api/orders
app.post('/api/orders', async (req, res) => {
  const { userId, restaurantId, items, address, paymentMethod } = req.body;
  
  // Validate
  if (!items || items.length === 0) {
    return res.status(400).json({ error: 'Giá» hÃ ng trá»‘ng' });
  }
  
  // Calculate total
  const total = items.reduce((sum, item) => sum + item.price * item.quantity, 0);
  
  // Create order
  const order = await Order.create({
    userId, restaurantId, items, address, paymentMethod,
    total, status: 'pending', createdAt: new Date()
  });
  
  res.status(201).json({ orderId: order.id, total, estimatedTime: '30-45 phÃºt' });
});
\`\`\`

---

## âœ… Phase D: Review (ÄÃ¡nh giÃ¡)
**YÃªu cáº§u:** "Review code API Ä‘áº·t hÃ ng á»Ÿ trÃªn"
**TÃ´i sáº½ Ä‘Æ°a ra:** ÄÃ¡nh giÃ¡ vÃ  cáº£i thiá»‡n.

### Code Review
| TiÃªu chÃ­ | Äiá»ƒm | Ghi chÃº |
|----------|------|---------|
| Logic | 8/10 | RÃµ rÃ ng, Ä‘Ãºng flow |
| Error Handling | 6/10 | Cáº§n thÃªm try-catch |
| Security | 5/10 | Thiáº¿u validate userId |
| Performance | 7/10 | OK cho MVP |

**Cáº§n cáº£i thiá»‡n:**
- ThÃªm authentication middleware
- Validate paymentMethod há»£p lá»‡
- ThÃªm transaction cho database

---

HÃ£y cho tÃ´i biáº¿t báº¡n cáº§n há»— trá»£ á»Ÿ Phase nÃ o!

## OUTPUT FORMAT
- ÄÆ°a DELIVERABLES thá»±c táº¿ (code, PRD, wireframe description, káº¿ hoáº¡ch...)
- Ngáº¯n gá»n, cÃ³ cáº¥u trÃºc, dá»… sá»­ dá»¥ng ngay
- Tráº£ lá»i báº±ng TIáº¾NG VIá»†T
- DÃ¹ng markdown formatting rÃµ rÃ ng`,

        en: `You are CVF Agent - an AI assistant following Controlled-Vibe Framework (CVF).

## CORE PRINCIPLES (MANDATORY!)
1. NO QUESTIONS - Make all assumptions based on best practices
2. NO PROCESS EXPLANATION - Don't mention "PHASE A", "Discovery", "Design"...
3. DELIVER FINAL RESULTS ONLY - User only needs deliverables, not process
4. ACT IMMEDIATELY - Don't wait for confirmation, don't list steps you'll take

## WHEN RECEIVING SPEC/REPORT
- DON'T summarize the spec (user already knows it)
- DON'T list steps you're going to take
- DIRECTLY provide results: code, documents, concrete plans...

## OUTPUT FORMAT
- Provide ACTUAL DELIVERABLES (code, PRD, wireframe description, plans...)
- Concise, structured, immediately usable
- Respond in ENGLISH
- Use clear markdown formatting`
    };

    return prompts[language];
}

// ==================== GEMINI PROVIDER ====================
export class GeminiProvider {
    private apiKey: string;
    private model: string;
    private baseUrl = 'https://generativelanguage.googleapis.com/v1beta';

    private language: 'vi' | 'en';

    constructor(config: AIProviderConfig) {
        this.apiKey = config.apiKey;
        this.model = config.model || 'gemini-2.5-flash';
        this.language = config.language || 'vi';
    }

    async chat(messages: AIMessage[], onStream?: (chunk: AIStreamChunk) => void): Promise<AIResponse> {
        const systemPrompt = getCVFSystemPrompt(this.language);

        // Build contents array
        const contents = messages.map(msg => ({
            role: msg.role === 'assistant' ? 'model' : 'user',
            parts: [{ text: msg.content }],
        }));

        // Add system instruction as first user message if needed
        if (systemPrompt && contents.length > 0) {
            contents[0].parts[0].text = `${systemPrompt}\n\n---\n\n${contents[0].parts[0].text}`;
        }

        const endpoint = onStream
            ? `${this.baseUrl}/models/${this.model}:streamGenerateContent?key=${this.apiKey}`
            : `${this.baseUrl}/models/${this.model}:generateContent?key=${this.apiKey}`;

        try {
            if (onStream) {
                return await this.streamRequest(endpoint, contents, onStream);
            } else {
                return await this.standardRequest(endpoint, contents);
            }
        } catch (error: unknown) {
            const errorMsg = error instanceof Error ? error.message : 'Unknown error';
            throw new Error(`Gemini API Error: ${errorMsg}`);
        }
    }

    private async standardRequest(endpoint: string, contents: Array<{ role: string; parts: { text: string }[] }>): Promise<AIResponse> {
        const response = await fetch(endpoint, {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({
                contents,
                generationConfig: {
                    temperature: 0.7,
                    maxOutputTokens: 8192,
                },
            }),
        });

        if (!response.ok) {
            const error = await response.json();
            throw new Error(error.error?.message || `HTTP ${response.status}`);
        }

        const data = await response.json();
        const text = data.candidates?.[0]?.content?.parts?.[0]?.text || '';

        return {
            text,
            model: this.model,
            usage: {
                promptTokens: data.usageMetadata?.promptTokenCount || 0,
                completionTokens: data.usageMetadata?.candidatesTokenCount || 0,
                totalTokens: data.usageMetadata?.totalTokenCount || 0,
            },
            finishReason: data.candidates?.[0]?.finishReason,
        };
    }

    private async streamRequest(
        endpoint: string,
        contents: Array<{ role: string; parts: { text: string }[] }>,
        onStream: (chunk: AIStreamChunk) => void
    ): Promise<AIResponse> {
        const response = await fetch(endpoint + '&alt=sse', {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({
                contents,
                generationConfig: {
                    temperature: 0.7,
                    maxOutputTokens: 8192,
                },
            }),
        });

        if (!response.ok) {
            const error = await response.json();
            throw new Error(error.error?.message || `HTTP ${response.status}`);
        }

        const reader = response.body?.getReader();
        if (!reader) throw new Error('No response body');

        const decoder = new TextDecoder();
        let fullText = '';
        let buffer = '';

        while (true) {
            const { done, value } = await reader.read();
            if (done) break;

            buffer += decoder.decode(value, { stream: true });
            const lines = buffer.split('\n');
            buffer = lines.pop() || '';

            for (const line of lines) {
                if (line.startsWith('data: ')) {
                    try {
                        const data = JSON.parse(line.slice(6));
                        const text = data.candidates?.[0]?.content?.parts?.[0]?.text || '';
                        if (text) {
                            fullText += text;
                            onStream({ text, isComplete: false });
                        }
                    } catch {
                        // Skip invalid JSON
                    }
                }
            }
        }

        onStream({ text: '', isComplete: true });

        return {
            text: fullText,
            model: this.model,
            finishReason: 'stop',
        };
    }
}

// ==================== OPENAI PROVIDER ====================
export class OpenAIProvider {
    private apiKey: string;
    private model: string;
    private baseUrl = 'https://api.openai.com/v1';

    private language: 'vi' | 'en';

    constructor(config: AIProviderConfig) {
        this.apiKey = config.apiKey;
        this.model = config.model || 'gpt-4o';
        this.language = config.language || 'vi';
    }

    async chat(messages: AIMessage[], onStream?: (chunk: AIStreamChunk) => void): Promise<AIResponse> {
        const systemMessage: AIMessage = { role: 'system', content: getCVFSystemPrompt(this.language) };
        const allMessages = [systemMessage, ...messages];

        const body = {
            model: this.model,
            messages: allMessages.map(m => ({ role: m.role, content: m.content })),
            temperature: 0.7,
            max_tokens: 4096,
            stream: !!onStream,
        };

        try {
            const response = await fetch(`${this.baseUrl}/chat/completions`, {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json',
                    'Authorization': `Bearer ${this.apiKey}`,
                },
                body: JSON.stringify(body),
            });

            if (!response.ok) {
                const error = await response.json();
                throw new Error(error.error?.message || `HTTP ${response.status}`);
            }

            if (onStream) {
                return await this.handleStream(response, onStream);
            } else {
                const data = await response.json();
                return {
                    text: data.choices[0]?.message?.content || '',
                    model: this.model,
                    usage: {
                        promptTokens: data.usage?.prompt_tokens || 0,
                        completionTokens: data.usage?.completion_tokens || 0,
                        totalTokens: data.usage?.total_tokens || 0,
                    },
                    finishReason: data.choices[0]?.finish_reason,
                };
            }
        } catch (error: unknown) {
            const errorMsg = error instanceof Error ? error.message : 'Unknown error';
            throw new Error(`OpenAI API Error: ${errorMsg}`);
        }
    }

    private async handleStream(response: Response, onStream: (chunk: AIStreamChunk) => void): Promise<AIResponse> {
        const reader = response.body?.getReader();
        if (!reader) throw new Error('No response body');

        const decoder = new TextDecoder();
        let fullText = '';
        let buffer = '';

        while (true) {
            const { done, value } = await reader.read();
            if (done) break;

            buffer += decoder.decode(value, { stream: true });
            const lines = buffer.split('\n');
            buffer = lines.pop() || '';

            for (const line of lines) {
                if (line.startsWith('data: ') && !line.includes('[DONE]')) {
                    try {
                        const data = JSON.parse(line.slice(6));
                        const text = data.choices?.[0]?.delta?.content || '';
                        if (text) {
                            fullText += text;
                            onStream({ text, isComplete: false });
                        }
                    } catch {
                        // Skip invalid JSON
                    }
                }
            }
        }

        onStream({ text: '', isComplete: true });

        return {
            text: fullText,
            model: this.model,
            finishReason: 'stop',
        };
    }
}

// ==================== ANTHROPIC PROVIDER ====================
export class AnthropicProvider {
    private apiKey: string;
    private model: string;
    private baseUrl = 'https://api.anthropic.com/v1';

    private language: 'vi' | 'en';

    constructor(config: AIProviderConfig) {
        this.apiKey = config.apiKey;
        this.model = config.model || 'claude-sonnet-4-20250514';
        this.language = config.language || 'vi';
    }

    async chat(messages: AIMessage[], onStream?: (chunk: AIStreamChunk) => void): Promise<AIResponse> {
        // Filter out system messages and use as system param
        const userMessages = messages.filter(m => m.role !== 'system');

        const body = {
            model: this.model,
            max_tokens: 4096,
            system: getCVFSystemPrompt(this.language),
            messages: userMessages.map(m => ({
                role: m.role as 'user' | 'assistant',
                content: m.content
            })),
            stream: !!onStream,
        };

        try {
            const response = await fetch(`${this.baseUrl}/messages`, {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json',
                    'x-api-key': this.apiKey,
                    'anthropic-version': '2023-06-01',
                    'anthropic-dangerous-direct-browser-access': 'true',
                },
                body: JSON.stringify(body),
            });

            if (!response.ok) {
                const error = await response.json();
                throw new Error(error.error?.message || `HTTP ${response.status}`);
            }

            if (onStream) {
                return await this.handleStream(response, onStream);
            } else {
                const data = await response.json();
                const text = data.content?.[0]?.text || '';
                return {
                    text,
                    model: this.model,
                    usage: {
                        promptTokens: data.usage?.input_tokens || 0,
                        completionTokens: data.usage?.output_tokens || 0,
                        totalTokens: (data.usage?.input_tokens || 0) + (data.usage?.output_tokens || 0),
                    },
                    finishReason: data.stop_reason,
                };
            }
        } catch (error: unknown) {
            const errorMsg = error instanceof Error ? error.message : 'Unknown error';
            throw new Error(`Anthropic API Error: ${errorMsg}`);
        }
    }

    private async handleStream(response: Response, onStream: (chunk: AIStreamChunk) => void): Promise<AIResponse> {
        const reader = response.body?.getReader();
        if (!reader) throw new Error('No response body');

        const decoder = new TextDecoder();
        let fullText = '';
        let buffer = '';

        while (true) {
            const { done, value } = await reader.read();
            if (done) break;

            buffer += decoder.decode(value, { stream: true });
            const lines = buffer.split('\n');
            buffer = lines.pop() || '';

            for (const line of lines) {
                if (line.startsWith('data: ')) {
                    try {
                        const data = JSON.parse(line.slice(6));
                        if (data.type === 'content_block_delta') {
                            const text = data.delta?.text || '';
                            if (text) {
                                fullText += text;
                                onStream({ text, isComplete: false });
                            }
                        }
                    } catch {
                        // Skip invalid JSON
                    }
                }
            }
        }

        onStream({ text: '', isComplete: true });

        return {
            text: fullText,
            model: this.model,
            finishReason: 'stop',
        };
    }
}

// ==================== UNIFIED PROVIDER ====================
export function createAIProvider(provider: AIProvider, config: AIProviderConfig) {
    if (MOCK_AI_ENABLED) {
        return new MockProvider(config);
    }
    switch (provider) {
        case 'gemini':
            return new GeminiProvider(config);
        case 'openai':
            return new OpenAIProvider(config);
        case 'anthropic':
            return new AnthropicProvider(config);
        default:
            throw new Error(`Unknown provider: ${provider}`);
    }
}

// Hook for using AI in components
import { useState, useCallback } from 'react';
import { useSettings } from '@/components/Settings';

export function useAI() {
    const { settings } = useSettings();
    const [isLoading, setIsLoading] = useState(false);
    const [error, setError] = useState<string | null>(null);

    const sendMessage = useCallback(async (
        messages: AIMessage[],
        onStream?: (chunk: AIStreamChunk) => void
    ): Promise<AIResponse | null> => {
        const provider = settings.preferences.defaultProvider;
        const apiKey = settings.providers[provider]?.apiKey;

        if (!apiKey) {
            setError('No API key configured');
            return null;
        }

        setIsLoading(true);
        setError(null);

        try {
            const aiProvider = createAIProvider(provider, { apiKey });
            const response = await aiProvider.chat(messages, onStream);
            return response;
        } catch (err: unknown) {
            const errorMsg = err instanceof Error ? err.message : 'Unknown error';
            setError(errorMsg);
            return null;
        } finally {
            setIsLoading(false);
        }
    }, [settings]);

    return {
        sendMessage,
        isLoading,
        error,
        provider: settings.preferences.defaultProvider,
        hasApiKey: !!settings.providers[settings.preferences.defaultProvider]?.apiKey,
    };
}
